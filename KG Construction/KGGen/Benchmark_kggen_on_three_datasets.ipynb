{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. ADE dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1111 update micro G-BERTScore\n",
    "# 1110 updated\n",
    "\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from kg_gen import KGGen\n",
    "import bert_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment  # For Hungarian matching\n",
    "import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "API_KEY = \"\"\n",
    "TEST_DATASET_PATH = '../datasets/ade_split_1_test_converted.json'\n",
    "MODEL_NAME = \"openai/gpt-4o\"\n",
    "SENTENCE_MODEL_NAME = 'all-mpnet-base-v2'  # Updated for semantic\n",
    "NUM_EXAMPLES_TO_TEST = None # whole test set\n",
    "\n",
    "# --- Paths ---\n",
    "current_date = datetime.datetime.now().strftime('%m%d')\n",
    "LOG_FILE_PATH = f'{current_date}_benchmark_kggen_fewshot_test_ade_1.log'\n",
    "FAILED_ENTRIES_PATH = f'{current_date}_ade_failed_entries.json'\n",
    "\n",
    "# --- Set up logging ---\n",
    "logging.basicConfig(level=logging.INFO, filename=LOG_FILE_PATH,\n",
    "                    filemode='a', format='%(message)s')\n",
    "\n",
    "# --- Main Script ---\n",
    "if not API_KEY or \"YOUR_API_KEY\" in API_KEY:\n",
    "    logging.warning(\"API key not set. Please use an environment variable.\")\n",
    "    print(\"WARNING: API key not found. Please set the API_KEY variable.\")\n",
    "\n",
    "kg = KGGen(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.1,\n",
    "    api_key=API_KEY\n",
    ")\n",
    "\n",
    "# --- Optimized Few-Shot Prompt (Further Refined: Added Emphasis on Flipping, Reduced Redundancy) ---\n",
    "EXTRACT_CONTEXT = \"\"\"\n",
    "You are a strict data conversion bot. Extract adverse drug events (ADEs) from scientific sentences into triples: (\"Specific Adverse Effect\", \"adverse_effect\", \"Drug/Chemical\").\n",
    "\n",
    "### CRITICAL RULES\n",
    "1. **RELATION:** Always use literal \"adverse_effect\" (lowercase). Only extract causal ADEs (drug causes effect); ignore others.\n",
    "2. **ORDER:** Always (Effect, \"adverse_effect\", Drug). Identify effect (symptom/condition) and drug (chemical). Flip order if extracted reversely.\n",
    "3. **OUTPUT:** ONLY unique triples list. Return [] if none. Avoid generics (e.g., \"patient\", ages), non-ADEs, negated/uncertain.\n",
    "- Use precise phrasing; trim unnecessary modifiers.\n",
    "- Focus on causation indicators (e.g., \"induced\", \"caused\", \"associated with\").\n",
    "\n",
    "Examples:\n",
    "\n",
    "Sentence: \"Cyclophosphamide is a human teratogen.\"\n",
    "Triples: [(\"human teratogen\", \"adverse_effect\", \"cyclophosphamide\")]\n",
    "\n",
    "Sentence: \"Lethal anuria complicating high dose ifosfamide chemotherapy.\"\n",
    "Triples: [(\"lethal anuria\", \"adverse_effect\", \"ifosfamide\")]\n",
    "\n",
    "Sentence: \"Gemcitabine-induced pulmonary toxicity.\"\n",
    "Triples: [(\"pulmonary toxicity\", \"adverse_effect\", \"gemcitabine\")]\n",
    "\n",
    "Sentence: \"The patient took aspirin for headache without issues.\"\n",
    "Triples: []\n",
    "\n",
    "Sentence: \"Senna caused subacute cholestatic hepatitis.\"\n",
    "Triples: [(\"subacute cholestatic hepatitis\", \"adverse_effect\", \"senna\")]\n",
    "\n",
    "Sentence: \"Fulminant hepatic failure associated with didanosine.\"\n",
    "Triples: [(\"fulminant hepatic failure\", \"adverse_effect\", \"didanosine\")]\n",
    "\n",
    "Sentence: \"A 34-year-old lady developed a constellation of dermatitis, fever, lymphadenopathy and hepatitis, beginning on the 17th day of a course of oral sulphasalazine for sero-negative rheumatoid arthritis.\"\n",
    "Triples: [(\"constellation of dermatitis\", \"adverse_effect\", \"sulphasalazine\"), (\"fever\", \"adverse_effect\", \"sulphasalazine\"), (\"lymphadenopathy\", \"adverse_effect\", \"sulphasalazine\"), (\"hepatitis\", \"adverse_effect\", \"sulphasalazine\")]\n",
    "\n",
    "Apply rules to the text. Extract triples only in (\"Effect\", \"adverse_effect\", \"Drug\") format.\n",
    "\"\"\"\n",
    "\n",
    "# --- Normalization Logic (Updated to match SciERC) ---\n",
    "def normalize_entity(entity_text: str) -> str:\n",
    "    if not isinstance(entity_text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = entity_text.strip().lower()\n",
    "    text = re.sub(r'^(a|an|the)\\s+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s*\\.$', '', text)\n",
    "    text = text.strip('\\'\"')\n",
    "    \n",
    "    return text if text else \"\"\n",
    "\n",
    "# --- Load dataset ---\n",
    "try:\n",
    "    with open(TEST_DATASET_PATH, 'r') as f:\n",
    "        data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"FATAL: Test dataset not found at path: {TEST_DATASET_PATH}\")\n",
    "    print(f\"FATAL: Test dataset not found at path: {TEST_DATASET_PATH}\")\n",
    "    exit()\n",
    "\n",
    "if NUM_EXAMPLES_TO_TEST:\n",
    "    data = data[:NUM_EXAMPLES_TO_TEST]\n",
    "\n",
    "print(f\"Loading sentence transformer model: {SENTENCE_MODEL_NAME}...\")\n",
    "sentence_model = SentenceTransformer(SENTENCE_MODEL_NAME)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# --- Benchmark Loop ---\n",
    "all_gold_triples, all_pred_triples, all_pred_strings, all_gold_strings = [], [], [], []\n",
    "item_details = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "failed_entries = []\n",
    "\n",
    "for i, item in enumerate(data):\n",
    "    text = item['text']\n",
    "    gold_triples_raw = item.get('triple_list', [])\n",
    "    \n",
    "    gold_triples = []\n",
    "    for subj, rel, obj in gold_triples_raw:\n",
    "        subj_norm = normalize_entity(subj)\n",
    "        obj_norm = normalize_entity(obj)\n",
    "        rel_norm = rel.strip().lower().replace(\" \", \"-\")\n",
    "        if subj_norm and obj_norm:\n",
    "            gold_triples.append((subj_norm, rel_norm, obj_norm))\n",
    "    gold_triples = sorted(list(set(gold_triples)))\n",
    "\n",
    "    logging.info(f\"\\n----- Processing Item {i+1}/{len(data)} -----\")\n",
    "    logging.info(f\"Text: {text}\")\n",
    "    print(f\"Processing Item {i+1}/{len(data)}...\")\n",
    "    \n",
    "    pred_triples = []\n",
    "    try:\n",
    "        # Single-Step Extraction with Few-Shot Prompt\n",
    "        response = kg.generate(input_data=text, context=EXTRACT_CONTEXT)\n",
    "        pred_triples_raw = response.relations if response and hasattr(response, 'relations') else []\n",
    "        logging.info(f\"KGGen Extraction: {pred_triples_raw}\")\n",
    "\n",
    "        corrected_triples_set = set()\n",
    "        for triple in pred_triples_raw:\n",
    "            if len(triple) != 3: continue\n",
    "            subj, rel, obj = triple\n",
    "            subj_norm = normalize_entity(subj)\n",
    "            obj_norm = normalize_entity(obj)\n",
    "            rel_norm = rel.strip().lower().replace(\" \", \"-\")\n",
    "            if subj_norm and obj_norm:\n",
    "                corrected_triples_set.add((subj_norm, rel_norm, obj_norm))\n",
    "\n",
    "        pred_triples = sorted(list(corrected_triples_set))\n",
    "        success_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An API or parsing error occurred for item {i+1}: {e}\", exc_info=True)\n",
    "        pred_triples = []\n",
    "        fail_count += 1\n",
    "        failed_entries.append({\n",
    "            'index': i+1,\n",
    "            'text': text,\n",
    "            'gold_triples_raw': gold_triples_raw,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Gold (Normalized): {gold_triples}\")\n",
    "    logging.info(f\"Pred (Normalized): {pred_triples}\")\n",
    "\n",
    "    all_gold_triples.extend(gold_triples)\n",
    "    all_pred_triples.extend(pred_triples)\n",
    "\n",
    "    pred_str = \" | \".join(sorted([f\"{h} {r} {t}\" for h, r, t in set(pred_triples)]))\n",
    "    gold_str = \" | \".join(sorted([f\"{h} {r} {t}\" for h, r, t in set(gold_triples)]))\n",
    "    all_pred_strings.append(pred_str)\n",
    "    all_gold_strings.append(gold_str)\n",
    "\n",
    "    item_details.append({\n",
    "        'index': i+1,\n",
    "        'gold_list': set(gold_triples), 'pred_list': set(pred_triples),\n",
    "        'gold_str': gold_str, 'pred_str': pred_str,\n",
    "        'gold_triples': gold_triples, 'pred_triples': pred_triples\n",
    "    })\n",
    "\n",
    "# Write failed entries to JSON\n",
    "if failed_entries:\n",
    "    with open(FAILED_ENTRIES_PATH, 'w') as f:\n",
    "        json.dump(failed_entries, f, indent=4)\n",
    "    logging.info(f\"Failed entries written to {FAILED_ENTRIES_PATH}\")\n",
    "else:\n",
    "    logging.info(\"No failed entries.\")\n",
    "\n",
    "# Log processing counts\n",
    "logging.info(f\"\\n----- Processing Summary -----\")\n",
    "logging.info(f\"Successful items: {success_count}\")\n",
    "logging.info(f\"Failed items: {fail_count}\")\n",
    "print(f\"Successful items: {success_count}\")\n",
    "print(f\"Failed items: {fail_count}\")\n",
    "\n",
    "# --- Metrics Calculation ---\n",
    "logging.info(\"\\n----- FINAL METRICS -----\")\n",
    "\n",
    "# Strict Metrics (Unchanged)\n",
    "gold_count = Counter(all_gold_triples)\n",
    "pred_count = Counter(all_pred_triples)\n",
    "tp = sum(min(gold_count[t], pred_count[t]) for t in gold_count)\n",
    "pred_total = len(all_pred_triples)\n",
    "gold_total = len(all_gold_triples)\n",
    "micro_p_strict = tp / pred_total if pred_total > 0 else 0.0\n",
    "micro_r_strict = tp / gold_total if gold_total > 0 else 0.0\n",
    "micro_f1_strict = 2 * micro_p_strict * micro_r_strict / \\\n",
    "    (micro_p_strict + micro_r_strict) if micro_p_strict + \\\n",
    "    micro_r_strict > 0 else 0.0\n",
    "\n",
    "item_ps_strict, item_rs_strict, item_f1s_strict = [], [], []\n",
    "for detail in item_details:\n",
    "    tp_item = len(detail['pred_list'] & detail['gold_list'])\n",
    "    p_item = tp_item / len(detail['pred_triples']) if detail['pred_triples'] else (1.0 if not detail['gold_triples'] else 0.0)\n",
    "    r_item = tp_item / len(detail['gold_triples']) if detail['gold_triples'] else (1.0 if not detail['pred_triples'] else 0.0)\n",
    "\n",
    "    if p_item + r_item == 0:\n",
    "        f1_item = 0.0\n",
    "    else:\n",
    "        f1_item = 2 * p_item * r_item / (p_item + r_item)\n",
    "\n",
    "    if not detail['pred_triples'] and not detail['gold_triples']:\n",
    "        f1_item = 1.0\n",
    "\n",
    "    item_ps_strict.append(p_item)\n",
    "    item_rs_strict.append(r_item)\n",
    "    item_f1s_strict.append(f1_item)\n",
    "    logging.info(\n",
    "        f\"Item {detail['index']} Strict: P={p_item:.4f}, R={r_item:.4f}, F1={f1_item:.4f}\")\n",
    "\n",
    "macro_p_strict = np.nanmean(item_ps_strict) if item_ps_strict else 0.0\n",
    "macro_r_strict = np.nanmean(item_rs_strict) if item_rs_strict else 0.0\n",
    "macro_f1_strict = np.nanmean(item_f1s_strict) if item_f1s_strict else 0.0\n",
    "\n",
    "# Base BERTScore (Unchanged, with IDF added for robustness)\n",
    "from bert_score import score as bert_score_compute\n",
    "\n",
    "if any(all_pred_strings) and any(all_gold_strings):\n",
    "    # Base BERTScore (P, R, F1) - Uses greedy max as per formulas\n",
    "    P_macro, R_macro, F1_macro = bert_score_compute(\n",
    "        all_pred_strings, all_gold_strings, lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=True\n",
    "    )\n",
    "    macro_p_bert = P_macro.mean().item()\n",
    "    macro_r_bert = R_macro.mean().item()\n",
    "    macro_f1_bert = F1_macro.mean().item()\n",
    "\n",
    "    all_pred_concat = ' | '.join(filter(None, all_pred_strings))\n",
    "    all_gold_concat = ' | '.join(filter(None, all_gold_strings))\n",
    "    P_micro, R_micro, F1_micro = bert_score_compute(\n",
    "        [all_pred_concat], [all_gold_concat], lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=False\n",
    "    )\n",
    "    micro_p_bert = P_micro.item()\n",
    "    micro_r_bert = R_micro.item() if not np.isnan(R_micro.item()) else 0.0\n",
    "    micro_f1_bert = F1_micro.item() if not np.isnan(F1_micro.item()) else 0.0\n",
    "    \n",
    "    # Greedy BERTScore (Renamed from G-BS)\n",
    "    item_gbs_p, item_gbs_r, item_gbs_f1 = [], [], []\n",
    "\n",
    "    for detail in item_details:\n",
    "        pred_strings = [f\"{h} {r} {t}\" for h, r, t in detail['pred_triples']]\n",
    "        gold_strings = [f\"{h} {r} {t}\" for h, r, t in detail['gold_triples']]\n",
    "\n",
    "        if not pred_strings and not gold_strings:\n",
    "            item_gbs_p.append(1.0)\n",
    "            item_gbs_r.append(1.0)\n",
    "            item_gbs_f1.append(1.0)\n",
    "            continue\n",
    "\n",
    "        if not pred_strings:  # Has gold, but no preds (all missed)\n",
    "            item_gbs_p.append(1.0)  # Vacuously true: all 0 preds are \"correct\"\n",
    "            item_gbs_r.append(0.0)\n",
    "            item_gbs_f1.append(0.0)\n",
    "            continue\n",
    "\n",
    "        if not gold_strings:  # Has preds, but no gold (all false pos)\n",
    "            item_gbs_p.append(0.0)\n",
    "            item_gbs_r.append(1.0)  # Vacuously true: all 0 golds are \"found\"\n",
    "            item_gbs_f1.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # G-BS-P (Precision): Avg. max similarity for each *predicted* triple\n",
    "        P_gbs, _, _ = bert_score_compute(\n",
    "            pred_strings,\n",
    "            [gold_strings] * len(pred_strings),  # Compare each pred to ALL golds\n",
    "            lang=\"en\", verbose=False,\n",
    "            model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        gbs_p = P_gbs.mean().item()\n",
    "\n",
    "        # G-BS-R (Recall): Avg. max similarity for each *gold* triple\n",
    "        P_recall, _, _ = bert_score_compute(  # Capture the first value (Precision)\n",
    "            gold_strings,\n",
    "            [pred_strings] * len(gold_strings),  # Compare each gold to ALL preds\n",
    "            lang=\"en\", verbose=False,\n",
    "            model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        gbs_r = P_recall.mean().item()  # Use the Precision from the flipped call\n",
    "        \n",
    "        gbs_f1 = 0.0\n",
    "        if (gbs_p + gbs_r) > 0:\n",
    "            gbs_f1 = 2 * gbs_p * gbs_r / (gbs_p + gbs_r)\n",
    "\n",
    "        item_gbs_p.append(gbs_p)\n",
    "        item_gbs_r.append(gbs_r)\n",
    "        item_gbs_f1.append(gbs_f1)\n",
    "\n",
    "    # Macro averages (mean over items)\n",
    "    macro_gbs_p = np.nanmean(item_gbs_p) if item_gbs_p else 0.0\n",
    "    macro_gbs_r = np.nanmean(item_gbs_r) if item_gbs_r else 0.0\n",
    "    macro_gbs_f1 = np.nanmean(item_gbs_f1) if item_gbs_f1 else 0.0\n",
    "\n",
    "    # Micro for Greedy BS: Flatten all triples and compute global greedy\n",
    "    all_pred_triple_strs = [f\"{h} {r} {t}\" for triples in item_details for h, r, t in triples['pred_triples']]\n",
    "    all_gold_triple_strs = [f\"{h} {r} {t}\" for triples in item_details for h, r, t in triples['gold_triples']]\n",
    "    \n",
    "    if all_pred_triple_strs and all_gold_triple_strs:\n",
    "        # Micro P: Avg max sim for each pred vs all golds\n",
    "        P_micro_g, _, _ = bert_score_compute(\n",
    "            all_pred_triple_strs,\n",
    "            [all_gold_triple_strs] * len(all_pred_triple_strs),\n",
    "            lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        micro_gbs_p = np.nanmean(P_micro_g.cpu().numpy())\n",
    "\n",
    "        # Micro R: Avg max sim for each gold vs all preds\n",
    "        P_micro_r, _, _ = bert_score_compute(  # Capture the first value (Precision)\n",
    "            all_gold_triple_strs,\n",
    "            [all_pred_triple_strs] * len(all_gold_triple_strs),\n",
    "            lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        micro_gbs_r = np.nanmean(P_micro_r.cpu().numpy())\n",
    "        \n",
    "        micro_gbs_f1 = 0.0\n",
    "        if (micro_gbs_p + micro_gbs_r) > 0:\n",
    "            micro_gbs_f1 = 2 * micro_gbs_p * micro_gbs_r / (micro_gbs_p + micro_gbs_r)\n",
    "    else:\n",
    "        micro_gbs_p, micro_gbs_r, micro_gbs_f1 = 0.0, 0.0, 0.0\n",
    "\n",
    "    # G-BERTScore (Hungarian Matching) from EXPLAGRAPHS\n",
    "    def get_g_bert_score(all_gold_edges, all_pred_edges, idf=False):\n",
    "        references = []\n",
    "        candidates = []\n",
    "        ref_cand_index = {}\n",
    "        for graph_idx, (gold_edges, pred_edges) in enumerate(zip(all_gold_edges, all_pred_edges)):\n",
    "            for gold_edge in gold_edges:\n",
    "                for pred_edge in pred_edges:\n",
    "                    references.append(gold_edge)\n",
    "                    candidates.append(pred_edge)\n",
    "                    ref_cand_index[(graph_idx, gold_edge, pred_edge)] = len(references) - 1\n",
    "\n",
    "        if not references:\n",
    "            return np.zeros(len(all_gold_edges)), np.zeros(len(all_gold_edges)), np.zeros(len(all_gold_edges))\n",
    "\n",
    "        _, _, bs_F1 = bert_score_compute(candidates, references, lang='en', verbose=False, model_type=\"roberta-large\", idf=idf)\n",
    "        bs_F1 = bs_F1.cpu().numpy()\n",
    "\n",
    "        precisions, recalls, f1s = [], [], []\n",
    "        for graph_idx, (gold_edges, pred_edges) in enumerate(zip(all_gold_edges, all_pred_edges)):\n",
    "            num_gold = len(gold_edges)\n",
    "            num_pred = len(pred_edges)\n",
    "            if num_gold == 0 and num_pred == 0:\n",
    "                precisions.append(1.0)\n",
    "                recalls.append(1.0)\n",
    "                f1s.append(1.0)\n",
    "                continue\n",
    "            if num_pred == 0:\n",
    "                precisions.append(1.0)\n",
    "                recalls.append(0.0)\n",
    "                f1s.append(0.0)\n",
    "                continue\n",
    "            if num_gold == 0:\n",
    "                precisions.append(0.0)\n",
    "                recalls.append(1.0)\n",
    "                f1s.append(0.0)\n",
    "                continue\n",
    "\n",
    "            score_matrix = np.zeros((num_gold, num_pred))\n",
    "            for i, gold_edge in enumerate(gold_edges):\n",
    "                for j, pred_edge in enumerate(pred_edges):\n",
    "                    idx = ref_cand_index.get((graph_idx, gold_edge, pred_edge))\n",
    "                    if idx is not None:\n",
    "                        score_matrix[i, j] = bs_F1[idx]\n",
    "\n",
    "            row_ind, col_ind = linear_sum_assignment(score_matrix, maximize=True)\n",
    "            matched_sim = score_matrix[row_ind, col_ind]\n",
    "\n",
    "            sample_precision = matched_sim.sum() / num_pred\n",
    "            sample_recall = matched_sim.sum() / num_gold\n",
    "            sample_f1 = 2 * sample_precision * sample_recall / (sample_precision + sample_recall) if sample_precision + sample_recall > 0 else 0.0\n",
    "\n",
    "            precisions.append(sample_precision)\n",
    "            recalls.append(sample_recall)\n",
    "            f1s.append(sample_f1)\n",
    "\n",
    "        return np.array(precisions), np.array(recalls), np.array(f1s)\n",
    "\n",
    "    all_gold_edges = [[f\"{h} {r} {t}\" for h, r, t in detail['gold_triples']] for detail in item_details]\n",
    "    all_pred_edges = [[f\"{h} {r} {t}\" for h, r, t in detail['pred_triples']] for detail in item_details]\n",
    "\n",
    "    precisions_h, recalls_h, f1s_h = get_g_bert_score(all_gold_edges, all_pred_edges, idf=False)\n",
    "\n",
    "    macro_gbs_p_h = np.nanmean(precisions_h) if len(precisions_h) > 0 else 0.0\n",
    "    macro_gbs_r_h = np.nanmean(recalls_h) if len(recalls_h) > 0 else 0.0\n",
    "    macro_gbs_f1_h = np.nanmean(f1s_h) if len(f1s_h) > 0 else 0.0\n",
    "\n",
    "    # Micro for G-BERTScore: Sum matched similarities from per-sample computations\n",
    "    total_matched_sum = 0.0\n",
    "    for i in range(len(precisions_h)):\n",
    "        total_matched_sum += precisions_h[i] * len(all_pred_edges[i])\n",
    "    micro_gbs_p_h = total_matched_sum / pred_total if pred_total > 0 else 0.0\n",
    "    micro_gbs_r_h = total_matched_sum / gold_total if gold_total > 0 else 0.0\n",
    "    micro_gbs_f1_h = 2 * micro_gbs_p_h * micro_gbs_r_h / (micro_gbs_p_h + micro_gbs_r_h) if micro_gbs_p_h + micro_gbs_r_h > 0 else 0.0\n",
    "    \n",
    "else:\n",
    "    # All zeros for empties\n",
    "    macro_p_bert, macro_r_bert, macro_f1_bert = 0.0, 0.0, 0.0\n",
    "    micro_p_bert, micro_r_bert, micro_f1_bert = 0.0, 0.0, 0.0\n",
    "    macro_gbs_p, macro_gbs_r, macro_gbs_f1 = 0.0, 0.0, 0.0\n",
    "    micro_gbs_p, micro_gbs_r, micro_gbs_f1 = 0.0, 0.0, 0.0\n",
    "    macro_gbs_p_h, macro_gbs_r_h, macro_gbs_f1_h = 0.0, 0.0, 0.0\n",
    "    micro_gbs_p_h, micro_gbs_r_h, micro_gbs_f1_h = 0.0, 0.0, 0.0\n",
    "\n",
    "# Soft Semantic Score (Unchanged)\n",
    "threshold = 0.8  # Keep as is, or tune per dataset (literature often uses 0.75-0.9)\n",
    "global_matched_preds, global_matched_golds = 0, 0\n",
    "item_ps_soft, item_rs_soft, item_f1s_soft = [], [], []\n",
    "\n",
    "for detail in item_details:\n",
    "    pred_triples, gold_triples = detail['pred_triples'], detail['gold_triples']\n",
    "    if not pred_triples and not gold_triples:\n",
    "        item_ps_soft.append(1.0)\n",
    "        item_rs_soft.append(1.0)\n",
    "        item_f1s_soft.append(1.0)\n",
    "        continue\n",
    "\n",
    "    pred_strings = [f\"{h} {r} {t}\" for h, r, t in pred_triples]\n",
    "    gold_strings = [f\"{h} {r} {t}\" for h, r, t in gold_triples]\n",
    "\n",
    "    if not pred_strings or not gold_strings:\n",
    "        matched_preds, matched_golds = 0, 0\n",
    "    else:\n",
    "        pred_embs = sentence_model.encode(pred_strings, convert_to_tensor=True).cpu().numpy()\n",
    "        gold_embs = sentence_model.encode(gold_strings, convert_to_tensor=True).cpu().numpy()\n",
    "        \n",
    "        # Cosine similarity matrix (normalize to [0,1] for matching)\n",
    "        sim_matrix = np.dot(pred_embs, gold_embs.T) / (np.linalg.norm(pred_embs, axis=1)[:, np.newaxis] * np.linalg.norm(gold_embs, axis=1))\n",
    "        sim_matrix = np.maximum(sim_matrix, 0)  # Clip negatives\n",
    "        \n",
    "        # Hungarian for optimal assignment (maximize similarity)\n",
    "        row_ind, col_ind = linear_sum_assignment(sim_matrix, maximize=True)\n",
    "        matched_sim = sim_matrix[row_ind, col_ind]\n",
    "        \n",
    "        # Count matches above threshold\n",
    "        matched_preds = sum(1 for s in matched_sim if s > threshold)\n",
    "        matched_golds = matched_preds  # Symmetric in bipartite\n",
    "\n",
    "    p_item = matched_preds / len(pred_triples) if pred_triples else (1.0 if not gold_triples else 0.0)\n",
    "    r_item = matched_golds / len(gold_triples) if gold_triples else (1.0 if not pred_triples else 0.0)\n",
    "    f1_item = 2 * p_item * r_item / (p_item + r_item) if p_item + r_item > 0 else 0.0\n",
    "\n",
    "    item_ps_soft.append(p_item)\n",
    "    item_rs_soft.append(r_item)\n",
    "    item_f1s_soft.append(f1_item)\n",
    "    global_matched_preds += matched_preds\n",
    "    global_matched_golds += matched_golds\n",
    "    logging.info(f\"Item {detail['index']} Semantic: P={p_item:.4f}, R={r_item:.4f}, F1={f1_item:.4f}\")\n",
    "\n",
    "micro_p_soft = global_matched_preds / pred_total if pred_total > 0 else 0.0\n",
    "micro_r_soft = global_matched_golds / gold_total if gold_total > 0 else 0.0\n",
    "micro_f1_soft = 2 * micro_p_soft * micro_r_soft / (micro_p_soft + micro_r_soft) if micro_p_soft + micro_r_soft > 0 else 0.0\n",
    "\n",
    "macro_p_soft = np.nanmean(item_ps_soft) if item_ps_soft else 0.0\n",
    "macro_r_soft = np.nanmean(item_rs_soft) if item_rs_soft else 0.0\n",
    "macro_f1_soft = np.nanmean(item_f1s_soft) if item_f1s_soft else 0.0\n",
    "\n",
    "# --- Final Output ---\n",
    "output_str = f\"\"\"\n",
    "Benchmark Results:\n",
    "---------------------------------\n",
    "Number of examples: {len(data)}\n",
    "Strict Averages:\n",
    "  - Macro: P={macro_p_strict:.4f}, R={macro_r_strict:.4f}, F1={macro_f1_strict:.4f}\n",
    "  - Micro: P={micro_p_strict:.4f}, R={micro_r_strict:.4f}, F1={micro_f1_strict:.4f}\n",
    "\n",
    "BERTScore Averages:\n",
    "  - Macro: P={macro_p_bert:.4f}, R={macro_r_bert:.4f}, F1={macro_f1_bert:.4f}\n",
    "  - Micro: P={micro_p_bert:.4f}, R={micro_r_bert:.4f}, F1={micro_f1_bert:.4f}\n",
    "\n",
    "Greedy BS Averages:\n",
    "  - Macro: P={macro_gbs_p:.4f}, R={macro_gbs_r:.4f}, F1={macro_gbs_f1:.4f}\n",
    "  - Micro: P={micro_gbs_p:.4f}, R={micro_gbs_r:.4f}, F1={micro_gbs_f1:.4f}\n",
    "\n",
    "G-BERTScore Averages:\n",
    "  - Macro: P={macro_gbs_p_h:.4f}, R={macro_gbs_r_h:.4f}, F1={macro_gbs_f1_h:.4f}\n",
    "  - Micro: P={micro_gbs_p_h:.4f}, R={micro_gbs_r_h:.4f}, F1={micro_gbs_f1_h:.4f}\n",
    "\n",
    "Semantic Score Averages (Threshold: {threshold}):\n",
    "  - Macro: P={macro_p_soft:.4f}, R={macro_r_soft:.4f}, F1={macro_f1_soft:.4f}\n",
    "  - Micro: P={micro_p_soft:.4f}, R={micro_r_soft:.4f}, F1={micro_f1_soft:.4f}\n",
    "\"\"\"\n",
    "print(output_str)\n",
    "logging.info(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. CONLL2004 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from kg_gen import KGGen\n",
    "import bert_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment  # For Hungarian matching\n",
    "import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "# It is a critical security risk to hardcode your API key. Please use an environment variable.\n",
    "# In your terminal, run: export OPENAI_API_KEY='your_real_api_key'\n",
    "API_KEY = \"\"\n",
    "# Using the path for the uploaded file\n",
    "DATASET_PATH = '../datasets/conll04_test_triples.json'\n",
    "current_date = datetime.datetime.now().strftime('%m%d')\n",
    "LOG_FILE_PATH = f'{current_date}_benchmark_conll04_test_all.log'\n",
    "MODEL_NAME = \"openai/gpt-4o\"\n",
    "SENTENCE_MODEL_NAME = 'all-mpnet-base-v2'\n",
    "# Set to None to run on the full dataset\n",
    "NUM_EXAMPLES_TO_TEST = None\n",
    "FAILED_ENTRIES_PATH = f'{current_date}_conll04_failed_entries.json'\n",
    "\n",
    "# --- Set up logging ---\n",
    "logging.basicConfig(level=logging.INFO, filename=LOG_FILE_PATH,\n",
    "                    filemode='a', format='%(message)s')\n",
    "\n",
    "# --- Main Script ---\n",
    "if not API_KEY.startswith(\"sk-proj-\"):\n",
    "    logging.warning(\n",
    "        \"OpenAI API key not set. The script will fail. Please set the OPENAI_API_KEY environment variable.\")\n",
    "    print(\"WARNING: OpenAI API key not found. Please set the OPENAI_API_KEY environment variable.\")\n",
    "\n",
    "kg = KGGen(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.1,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "# --- Stricter Prompt ---\n",
    "CONTEXT = \"\"\"You MUST extract relationships ONLY using these exact relation names: kill, work_for, organization_based_in, live_in, located_in. You MUST NOT use any other relation namesâ€”any deviation is invalid. Subject and object MUST be complete noun phrases from the text. Output ONLY unique triples as (Subject, relation, Object) or [] if none. Infer relations where implied (e.g., 'born in' implies 'live_in'; 'director of' implies 'work_for').\n",
    "\n",
    "Invalid Example (Do NOT do this):\n",
    "Sentence: \"Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing.\"\n",
    "Wrong Triples: [(\"Recognition\", \"has-been-studied-as\", \"part\"), (\"Recognition\", \"of\", \"proper nouns\")]  # Invalid relations\n",
    "\n",
    "Valid Examples:\n",
    "Sentence: \"Newspaper ` Explains ' U.S. Interests Section Events FL1402001894 Havana Radio Reloj Network in Spanish 2100 GMT 13 Feb 94\"\n",
    "Triples: [(\"radio reloj network\", \"organization_based_in\", \"havana\")]\n",
    "\n",
    "Sentence: \"Annie Oakley , also known as Little Miss Sure Shot , was born Phoebe Ann Moses in Willowdell , Darke County , in 1860 .\"\n",
    "Triples: [(\"annie oakley\", \"live_in\", \"willowdell , darke county\"), (\"little miss sure shot\", \"live_in\", \"willowdell , darke county\"), (\"phoebe ann moses\", \"live_in\", \"willowdell , darke county\")]\n",
    "\n",
    "Apply to the text. Remember: ONLY use relations from the list.\"\"\"\n",
    "\n",
    "# --- Normalization Logic ---\n",
    "def normalize_entity(entity_text: str) -> str:\n",
    "    if not isinstance(entity_text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = entity_text.strip().lower()\n",
    "    text = re.sub(r'^(a|an|the)\\s+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s*\\.$', '', text)\n",
    "    text = text.strip('\\'\"')\n",
    "    \n",
    "    return text if text else \"\"\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    with open(DATASET_PATH, 'r') as f:\n",
    "        data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"FATAL: Dataset not found at path: {DATASET_PATH}\")\n",
    "    print(f\"FATAL: Dataset not found at path: {DATASET_PATH}\")\n",
    "    exit()\n",
    "\n",
    "if NUM_EXAMPLES_TO_TEST:\n",
    "    data = data[:NUM_EXAMPLES_TO_TEST]\n",
    "\n",
    "print(f\"Loading sentence transformer model: {SENTENCE_MODEL_NAME}...\")\n",
    "sentence_model = SentenceTransformer(SENTENCE_MODEL_NAME)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# --- Benchmark Loop ---\n",
    "all_gold_triples, all_pred_triples, all_pred_strings, all_gold_strings = [], [], [], []\n",
    "item_details = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "failed_entries = []\n",
    "\n",
    "for i, item in enumerate(data):\n",
    "    text = item['text']\n",
    "    gold_triples_raw = item.get('triple_list', [])\n",
    "    \n",
    "    gold_triples = []\n",
    "    for subj, pred, obj in gold_triples_raw:\n",
    "        subj_norm = normalize_entity(subj)\n",
    "        obj_norm = normalize_entity(obj)\n",
    "        pred_norm = pred.strip().lower().replace(\" \", \"_\")\n",
    "        if subj_norm and obj_norm:\n",
    "            gold_triples.append((subj_norm, pred_norm, obj_norm))\n",
    "    gold_triples = sorted(list(set(gold_triples)))\n",
    "\n",
    "    logging.info(f\"\\n----- Processing Item {i+1}/{len(data)} -----\")\n",
    "    logging.info(f\"Text: {text}\")\n",
    "    print(f\"Processing Item {i+1}/{len(data)}...\")\n",
    "    \n",
    "    pred_triples = []\n",
    "    try:\n",
    "        response = kg.generate(\n",
    "            input_data=text, \n",
    "            context=CONTEXT\n",
    "        )\n",
    "        pred_triples_raw = response.relations if response and hasattr(response, 'relations') else []\n",
    "        logging.info(f\"KGGen Extraction: {pred_triples_raw}\")\n",
    "\n",
    "        corrected_triples_set = set()\n",
    "        for triple in pred_triples_raw:\n",
    "            if len(triple) != 3: continue\n",
    "            subj, pred, obj = triple\n",
    "            subj_norm = normalize_entity(subj)\n",
    "            obj_norm = normalize_entity(obj)\n",
    "            pred_norm = pred.strip().lower().replace(\" \", \"_\")\n",
    "            if subj_norm and obj_norm:\n",
    "                corrected_triples_set.add((subj_norm, pred_norm, obj_norm))\n",
    "\n",
    "        pred_triples = sorted(list(corrected_triples_set))\n",
    "        success_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An API or parsing error occurred for item {i+1}: {e}\", exc_info=True)\n",
    "        pred_triples = []\n",
    "        fail_count += 1\n",
    "        failed_entries.append({\n",
    "            'index': i+1,\n",
    "            'text': text,\n",
    "            'gold_triples_raw': gold_triples_raw,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Gold (Normalized): {gold_triples}\")\n",
    "    logging.info(f\"Pred (Normalized): {pred_triples}\")\n",
    "\n",
    "    all_gold_triples.extend(gold_triples)\n",
    "    all_pred_triples.extend(pred_triples)\n",
    "\n",
    "    pred_str = \" | \".join(sorted([f\"{h} {r} {t}\" for h, r, t in set(pred_triples)]))\n",
    "    gold_str = \" | \".join(sorted([f\"{h} {r} {t}\" for h, r, t in set(gold_triples)]))\n",
    "    all_pred_strings.append(pred_str)\n",
    "    all_gold_strings.append(gold_str)\n",
    "\n",
    "    item_details.append({\n",
    "        'index': i+1,\n",
    "        'gold_list': set(gold_triples), 'pred_list': set(pred_triples),\n",
    "        'gold_str': gold_str, 'pred_str': pred_str,\n",
    "        'gold_triples': gold_triples, 'pred_triples': pred_triples\n",
    "    })\n",
    "\n",
    "# Write failed entries to JSON\n",
    "if failed_entries:\n",
    "    with open(FAILED_ENTRIES_PATH, 'w') as f:\n",
    "        json.dump(failed_entries, f, indent=4)\n",
    "    logging.info(f\"Failed entries written to {FAILED_ENTRIES_PATH}\")\n",
    "else:\n",
    "    logging.info(\"No failed entries.\")\n",
    "\n",
    "# Log processing counts\n",
    "logging.info(f\"\\n----- Processing Summary -----\")\n",
    "logging.info(f\"Successful items: {success_count}\")\n",
    "logging.info(f\"Failed items: {fail_count}\")\n",
    "print(f\"Successful items: {success_count}\")\n",
    "print(f\"Failed items: {fail_count}\")\n",
    "\n",
    "# --- Metrics Calculation ---\n",
    "logging.info(\"\\n----- FINAL METRICS -----\")\n",
    "\n",
    "# Strict Metrics\n",
    "gold_count = Counter(all_gold_triples)\n",
    "pred_count = Counter(all_pred_triples)\n",
    "tp = sum(min(gold_count[t], pred_count[t]) for t in gold_count)\n",
    "pred_total = len(all_pred_triples)\n",
    "gold_total = len(all_gold_triples)\n",
    "micro_p_strict = tp / pred_total if pred_total > 0 else 0.0\n",
    "micro_r_strict = tp / gold_total if gold_total > 0 else 0.0\n",
    "micro_f1_strict = 2 * micro_p_strict * micro_r_strict / \\\n",
    "    (micro_p_strict + micro_r_strict) if micro_p_strict + \\\n",
    "    micro_r_strict > 0 else 0.0\n",
    "\n",
    "item_ps_strict, item_rs_strict, item_f1s_strict = [], [], []\n",
    "for detail in item_details:\n",
    "    tp_item = len(detail['pred_list'] & detail['gold_list'])\n",
    "    p_item = tp_item / len(detail['pred_triples']) if detail['pred_triples'] else (1.0 if not detail['gold_triples'] else 0.0)\n",
    "    r_item = tp_item / len(detail['gold_triples']) if detail['gold_triples'] else (1.0 if not detail['pred_triples'] else 0.0)\n",
    "    if p_item + r_item == 0:\n",
    "        f1_item = 0.0\n",
    "    else:\n",
    "        f1_item = 2 * p_item * r_item / (p_item + r_item)\n",
    "    if not detail['pred_triples'] and not detail['gold_triples']:\n",
    "        f1_item = 1.0\n",
    "    item_ps_strict.append(p_item)\n",
    "    item_rs_strict.append(r_item)\n",
    "    item_f1s_strict.append(f1_item)\n",
    "    logging.info(\n",
    "        f\"Item {detail['index']} Strict: P={p_item:.4f}, R={r_item:.4f}, F1={f1_item:.4f}\")\n",
    "\n",
    "macro_p_strict = np.nanmean(item_ps_strict) if item_ps_strict else 0.0\n",
    "macro_r_strict = np.nanmean(item_rs_strict) if item_rs_strict else 0.0\n",
    "macro_f1_strict = np.nanmean(item_f1s_strict) if item_f1s_strict else 0.0\n",
    "\n",
    "# Base BERTScore (with IDF added for robustness)\n",
    "from bert_score import score as bert_score_compute\n",
    "\n",
    "if any(all_pred_strings) and any(all_gold_strings):\n",
    "    # Base BERTScore (P, R, F1) - Uses greedy max as per formulas\n",
    "    P_macro, R_macro, F1_macro = bert_score_compute(\n",
    "        all_pred_strings, all_gold_strings, lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=True\n",
    "    )\n",
    "    macro_p_bert = P_macro.mean().item()\n",
    "    macro_r_bert = R_macro.mean().item()\n",
    "    macro_f1_bert = F1_macro.mean().item()\n",
    "\n",
    "    all_pred_concat = ' | '.join(filter(None, all_pred_strings))\n",
    "    all_gold_concat = ' | '.join(filter(None, all_gold_strings))\n",
    "    P_micro, R_micro, F1_micro = bert_score_compute(\n",
    "        [all_pred_concat], [all_gold_concat], lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=False\n",
    "    )\n",
    "    micro_p_bert = P_micro.item()\n",
    "    micro_r_bert = R_micro.item() if not np.isnan(R_micro.item()) else 0.0\n",
    "    micro_f1_bert = F1_micro.item() if not np.isnan(F1_micro.item()) else 0.0\n",
    "    \n",
    "    # Greedy BERTScore\n",
    "    item_gbs_p, item_gbs_r, item_gbs_f1 = [], [], []\n",
    "\n",
    "    for detail in item_details:\n",
    "        pred_strings = [f\"{h} {r} {t}\" for h, r, t in detail['pred_triples']]\n",
    "        gold_strings = [f\"{h} {r} {t}\" for h, r, t in detail['gold_triples']]\n",
    "\n",
    "        if not pred_strings and not gold_strings:\n",
    "            item_gbs_p.append(1.0)\n",
    "            item_gbs_r.append(1.0)\n",
    "            item_gbs_f1.append(1.0)\n",
    "            continue\n",
    "\n",
    "        if not pred_strings:  # Has gold, but no preds (all missed)\n",
    "            item_gbs_p.append(1.0)  # Vacuously true: all 0 preds are \"correct\"\n",
    "            item_gbs_r.append(0.0)\n",
    "            item_gbs_f1.append(0.0)\n",
    "            continue\n",
    "\n",
    "        if not gold_strings:  # Has preds, but no gold (all false pos)\n",
    "            item_gbs_p.append(0.0)\n",
    "            item_gbs_r.append(1.0)  # Vacuously true: all 0 golds are \"found\"\n",
    "            item_gbs_f1.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # G-BS-P (Precision): Avg. max similarity for each *predicted* triple\n",
    "        P_gbs, _, _ = bert_score_compute(\n",
    "            pred_strings,\n",
    "            [gold_strings] * len(pred_strings),  # Compare each pred to ALL golds\n",
    "            lang=\"en\", verbose=False,\n",
    "            model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        gbs_p = P_gbs.mean().item()\n",
    "\n",
    "        # G-BS-R (Recall): Avg. max similarity for each *gold* triple\n",
    "        P_recall, _, _ = bert_score_compute(  # Capture the first value (Precision)\n",
    "            gold_strings,\n",
    "            [pred_strings] * len(gold_strings),  # Compare each gold to ALL preds\n",
    "            lang=\"en\", verbose=False,\n",
    "            model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        gbs_r = P_recall.mean().item()  # Use the Precision from the flipped call\n",
    "        \n",
    "        gbs_f1 = 0.0\n",
    "        if (gbs_p + gbs_r) > 0:\n",
    "            gbs_f1 = 2 * gbs_p * gbs_r / (gbs_p + gbs_r)\n",
    "\n",
    "        item_gbs_p.append(gbs_p)\n",
    "        item_gbs_r.append(gbs_r)\n",
    "        item_gbs_f1.append(gbs_f1)\n",
    "\n",
    "    # Macro averages (mean over items)\n",
    "    macro_gbs_p = np.nanmean(item_gbs_p) if item_gbs_p else 0.0\n",
    "    macro_gbs_r = np.nanmean(item_gbs_r) if item_gbs_r else 0.0\n",
    "    macro_gbs_f1 = np.nanmean(item_gbs_f1) if item_gbs_f1 else 0.0\n",
    "\n",
    "    # Micro for G-BS: Flatten all triples and compute global greedy\n",
    "    all_pred_triple_strs = [f\"{h} {r} {t}\" for triples in item_details for h, r, t in triples['pred_triples']]\n",
    "    all_gold_triple_strs = [f\"{h} {r} {t}\" for triples in item_details for h, r, t in triples['gold_triples']]\n",
    "    \n",
    "    if all_pred_triple_strs and all_gold_triple_strs:\n",
    "        # Micro P: Avg max sim for each pred vs all golds\n",
    "        P_micro_g, _, _ = bert_score_compute(\n",
    "            all_pred_triple_strs,\n",
    "            [all_gold_triple_strs] * len(all_pred_triple_strs),\n",
    "            lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        micro_gbs_p = np.nanmean(P_micro_g.cpu().numpy())\n",
    "\n",
    "        # Micro R: Avg max sim for each gold vs all preds\n",
    "        P_micro_r, _, _ = bert_score_compute(  # Capture the first value (Precision)\n",
    "            all_gold_triple_strs,\n",
    "            [all_pred_triple_strs] * len(all_gold_triple_strs),\n",
    "            lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        micro_gbs_r = np.nanmean(P_micro_r.cpu().numpy())\n",
    "        \n",
    "        micro_gbs_f1 = 0.0\n",
    "        if (micro_gbs_p + micro_gbs_r) > 0:\n",
    "            micro_gbs_f1 = 2 * micro_gbs_p * micro_gbs_r / (micro_gbs_p + micro_gbs_r)\n",
    "    else:\n",
    "        micro_gbs_p, micro_gbs_r, micro_gbs_f1 = 0.0, 0.0, 0.0\n",
    "    \n",
    "else:\n",
    "    # All zeros for empties\n",
    "    macro_p_bert, macro_r_bert, macro_f1_bert = 0.0, 0.0, 0.0\n",
    "    micro_p_bert, micro_r_bert, micro_f1_bert = 0.0, 0.0, 0.0\n",
    "    macro_gbs_p, macro_gbs_r, macro_gbs_f1 = 0.0, 0.0, 0.0\n",
    "    micro_gbs_p, micro_gbs_r, micro_gbs_f1 = 0.0, 0.0, 0.0\n",
    "\n",
    "# G-BERTScore (Hungarian Matching) from EXPLAGRAPHS\n",
    "def get_g_bert_score(all_gold_edges, all_pred_edges, idf=False):\n",
    "    references = []\n",
    "    candidates = []\n",
    "    ref_cand_index = {}\n",
    "    for graph_idx, (gold_edges, pred_edges) in enumerate(zip(all_gold_edges, all_pred_edges)):\n",
    "        for gold_edge in gold_edges:\n",
    "            for pred_edge in pred_edges:\n",
    "                references.append(gold_edge)\n",
    "                candidates.append(pred_edge)\n",
    "                ref_cand_index[(graph_idx, gold_edge, pred_edge)] = len(references) - 1\n",
    "\n",
    "    if not references:\n",
    "        return np.zeros(len(all_gold_edges)), np.zeros(len(all_gold_edges)), np.zeros(len(all_gold_edges))\n",
    "\n",
    "    _, _, bs_F1 = bert_score_compute(candidates, references, lang='en', verbose=False, model_type=\"roberta-large\", idf=idf)\n",
    "    bs_F1 = bs_F1.cpu().numpy()\n",
    "\n",
    "    precisions, recalls, f1s = [], [], []\n",
    "    for graph_idx, (gold_edges, pred_edges) in enumerate(zip(all_gold_edges, all_pred_edges)):\n",
    "        num_gold = len(gold_edges)\n",
    "        num_pred = len(pred_edges)\n",
    "        if num_gold == 0 and num_pred == 0:\n",
    "            precisions.append(1.0)\n",
    "            recalls.append(1.0)\n",
    "            f1s.append(1.0)\n",
    "            continue\n",
    "        if num_pred == 0:\n",
    "            precisions.append(1.0)\n",
    "            recalls.append(0.0)\n",
    "            f1s.append(0.0)\n",
    "            continue\n",
    "        if num_gold == 0:\n",
    "            precisions.append(0.0)\n",
    "            recalls.append(1.0)\n",
    "            f1s.append(0.0)\n",
    "            continue\n",
    "\n",
    "        score_matrix = np.zeros((num_gold, num_pred))\n",
    "        for i, gold_edge in enumerate(gold_edges):\n",
    "            for j, pred_edge in enumerate(pred_edges):\n",
    "                idx = ref_cand_index.get((graph_idx, gold_edge, pred_edge))\n",
    "                if idx is not None:\n",
    "                    score_matrix[i, j] = bs_F1[idx]\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(score_matrix, maximize=True)\n",
    "        matched_sim = score_matrix[row_ind, col_ind]\n",
    "\n",
    "        sample_precision = matched_sim.sum() / num_pred\n",
    "        sample_recall = matched_sim.sum() / num_gold\n",
    "        sample_f1 = 2 * sample_precision * sample_recall / (sample_precision + sample_recall) if sample_precision + sample_recall > 0 else 0.0\n",
    "\n",
    "        precisions.append(sample_precision)\n",
    "        recalls.append(sample_recall)\n",
    "        f1s.append(sample_f1)\n",
    "\n",
    "    return np.array(precisions), np.array(recalls), np.array(f1s)\n",
    "\n",
    "all_gold_edges = [[f\"{h} {r} {t}\" for h, r, t in detail['gold_triples']] for detail in item_details]\n",
    "all_pred_edges = [[f\"{h} {r} {t}\" for h, r, t in detail['pred_triples']] for detail in item_details]\n",
    "\n",
    "precisions_h, recalls_h, f1s_h = get_g_bert_score(all_gold_edges, all_pred_edges, idf=False)\n",
    "\n",
    "macro_gbs_p_h = np.nanmean(precisions_h) if len(precisions_h) > 0 else 0.0\n",
    "macro_gbs_r_h = np.nanmean(recalls_h) if len(recalls_h) > 0 else 0.0\n",
    "macro_gbs_f1_h = np.nanmean(f1s_h) if len(f1s_h) > 0 else 0.0\n",
    "\n",
    "# Micro for G-BERTScore: Sum matched similarities from per-sample computations\n",
    "total_matched_sum = 0.0\n",
    "for i in range(len(precisions_h)):\n",
    "    total_matched_sum += precisions_h[i] * len(all_pred_edges[i])\n",
    "micro_gbs_p_h = total_matched_sum / pred_total if pred_total > 0 else 0.0\n",
    "micro_gbs_r_h = total_matched_sum / gold_total if gold_total > 0 else 0.0\n",
    "micro_gbs_f1_h = 2 * micro_gbs_p_h * micro_gbs_r_h / (micro_gbs_p_h + micro_gbs_r_h) if micro_gbs_p_h + micro_gbs_r_h > 0 else 0.0\n",
    "\n",
    "# Soft Semantic Score\n",
    "threshold = 0.8  # Keep as is, or tune per dataset (literature often uses 0.75-0.9)\n",
    "global_matched_preds, global_matched_golds = 0, 0\n",
    "item_ps_soft, item_rs_soft, item_f1s_soft = [], [], []\n",
    "\n",
    "for detail in item_details:\n",
    "    pred_triples, gold_triples = detail['pred_triples'], detail['gold_triples']\n",
    "    if not pred_triples and not gold_triples:\n",
    "        item_ps_soft.append(1.0)\n",
    "        item_rs_soft.append(1.0)\n",
    "        item_f1s_soft.append(1.0)\n",
    "        continue\n",
    "\n",
    "    pred_strings = [f\"{h} {r} {t}\" for h, r, t in pred_triples]\n",
    "    gold_strings = [f\"{h} {r} {t}\" for h, r, t in gold_triples]\n",
    "\n",
    "    if not pred_strings or not gold_strings:\n",
    "        matched_preds, matched_golds = 0, 0\n",
    "    else:\n",
    "        pred_embs = sentence_model.encode(pred_strings, convert_to_tensor=True).cpu().numpy()\n",
    "        gold_embs = sentence_model.encode(gold_strings, convert_to_tensor=True).cpu().numpy()\n",
    "        \n",
    "        # Cosine similarity matrix (normalize to [0,1] for matching)\n",
    "        sim_matrix = np.dot(pred_embs, gold_embs.T) / (np.linalg.norm(pred_embs, axis=1)[:, np.newaxis] * np.linalg.norm(gold_embs, axis=1))\n",
    "        sim_matrix = np.maximum(sim_matrix, 0)  # Clip negatives\n",
    "        \n",
    "        # Hungarian for optimal assignment (maximize similarity)\n",
    "        row_ind, col_ind = linear_sum_assignment(sim_matrix, maximize=True)\n",
    "        matched_sim = sim_matrix[row_ind, col_ind]\n",
    "        \n",
    "        # Count matches above threshold\n",
    "        matched_preds = sum(1 for s in matched_sim if s > threshold)\n",
    "        matched_golds = matched_preds  # Symmetric in bipartite\n",
    "\n",
    "    p_item = matched_preds / len(pred_triples) if pred_triples else (1.0 if not gold_triples else 0.0)\n",
    "    r_item = matched_golds / len(gold_triples) if gold_triples else (1.0 if not pred_triples else 0.0)\n",
    "    f1_item = 2 * p_item * r_item / (p_item + r_item) if p_item + r_item > 0 else 0.0\n",
    "\n",
    "    item_ps_soft.append(p_item)\n",
    "    item_rs_soft.append(r_item)\n",
    "    item_f1s_soft.append(f1_item)\n",
    "    global_matched_preds += matched_preds\n",
    "    global_matched_golds += matched_golds\n",
    "    logging.info(f\"Item {detail['index']} Semantic: P={p_item:.4f}, R={r_item:.4f}, F1={f1_item:.4f}\")\n",
    "\n",
    "micro_p_soft = global_matched_preds / pred_total if pred_total > 0 else 0.0\n",
    "micro_r_soft = global_matched_golds / gold_total if gold_total > 0 else 0.0\n",
    "micro_f1_soft = 2 * micro_p_soft * micro_r_soft / (micro_p_soft + micro_r_soft) if micro_p_soft + micro_r_soft > 0 else 0.0\n",
    "\n",
    "macro_p_soft = np.nanmean(item_ps_soft) if item_ps_soft else 0.0\n",
    "macro_r_soft = np.nanmean(item_rs_soft) if item_rs_soft else 0.0\n",
    "macro_f1_soft = np.nanmean(item_f1s_soft) if item_f1s_soft else 0.0\n",
    "\n",
    "# --- Final Output ---\n",
    "output_str = f\"\"\"\n",
    "Benchmark Results:\n",
    "---------------------------------\n",
    "Number of examples: {len(data)}\n",
    "Strict Averages:\n",
    "  - Macro: P={macro_p_strict:.4f}, R={macro_r_strict:.4f}, F1={macro_f1_strict:.4f}\n",
    "  - Micro: P={micro_p_strict:.4f}, R={micro_r_strict:.4f}, F1={micro_f1_strict:.4f}\n",
    "\n",
    "BERTScore Averages:\n",
    "  - Macro: P={macro_p_bert:.4f}, R={macro_r_bert:.4f}, F1={macro_f1_bert:.4f}\n",
    "  - Micro: P={micro_p_bert:.4f}, R={micro_r_bert:.4f}, F1={micro_f1_bert:.4f}\n",
    "\n",
    "Greedy BS Averages:\n",
    "  - Macro: P={macro_gbs_p:.4f}, R={macro_gbs_r:.4f}, F1={macro_gbs_f1:.4f}\n",
    "  - Micro: P={micro_gbs_p:.4f}, R={micro_gbs_r:.4f}, F1={micro_gbs_f1:.4f}\n",
    "\n",
    "G-BERTScore Averages:\n",
    "  - Macro: P={macro_gbs_p_h:.4f}, R={macro_gbs_r_h:.4f}, F1={macro_gbs_f1_h:.4f}\n",
    "  - Micro: P={micro_gbs_p_h:.4f}, R={micro_gbs_r_h:.4f}, F1={micro_gbs_f1_h:.4f}\n",
    "\n",
    "Semantic Score Averages (Threshold: {threshold}):\n",
    "  - Macro: P={macro_p_soft:.4f}, R={macro_r_soft:.4f}, F1={macro_f1_soft:.4f}\n",
    "  - Micro: P={micro_p_soft:.4f}, R={micro_r_soft:.4f}, F1={micro_f1_soft:.4f}\n",
    "\"\"\"\n",
    "print(output_str)\n",
    "logging.info(output_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. SciERC datatset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "import logging\n",
    "import os\n",
    "from collections import Counter\n",
    "from typing import List, Tuple, Dict, Set\n",
    "from kg_gen import KGGen\n",
    "import bert_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.util import cos_sim\n",
    "import torch\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy.optimize import linear_sum_assignment  # For Hungarian matching\n",
    "import datetime\n",
    "\n",
    "# --- Configuration ---\n",
    "API_KEY = \"\"\n",
    "\n",
    "# --- Paths ---\n",
    "current_date = datetime.datetime.now().strftime('%m%d')\n",
    "LOG_FILE_PATH = f'{current_date}_benchmark_scierc_test_all.log'\n",
    "DATASET_PATH = '../datasets/scierc_test_converted.json'\n",
    "MODEL_NAME = \"openai/gpt-4o\"\n",
    "SENTENCE_MODEL_NAME = 'all-mpnet-base-v2'\n",
    "NUM_EXAMPLES_TO_TEST = None\n",
    "FAILED_ENTRIES_PATH = f'{current_date}_scierc_failed_entries.json'\n",
    "\n",
    "# --- Set up logging ---\n",
    "logging.basicConfig(level=logging.INFO, filename=LOG_FILE_PATH,\n",
    "                    filemode='a', format='%(message)s')\n",
    "\n",
    "# --- Main Script ---\n",
    "if not API_KEY.startswith(\"sk-proj-\"):\n",
    "    logging.warning(\"OpenAI API key not set.\")\n",
    "    print(\"WARNING: OpenAI API key not set.\")\n",
    "\n",
    "kg = KGGen(\n",
    "    model=MODEL_NAME,\n",
    "    temperature=0.1,\n",
    "    api_key=API_KEY,\n",
    ")\n",
    "\n",
    "# --- Stricter Prompt ---\n",
    "CONTEXT = \"\"\"You MUST extract relationships ONLY using these exact relation names: used-for, feature-of, hyponym-of, part-of, compare, evaluate-for, conjunction. You MUST NOT use any other relation namesâ€”any deviation is invalid. Subject and object MUST be complete noun phrases from the text. Output ONLY unique triples as (Subject, relation, Object) or [] if none. Do NOT use relations like 'has-been-studied-as', 'of', 'in'â€”those are invalid; stick ONLY to the list.\n",
    "\n",
    "Invalid Example (Do NOT do this):\n",
    "Sentence: \"Recognition of proper nouns in Japanese text has been studied as a part of the more general problem of morphological analysis in Japanese text processing.\"\n",
    "Wrong Triples: [(\"Recognition\", \"has-been-studied-as\", \"part\"), (\"Recognition\", \"of\", \"proper nouns\")]  # Invalid relations\n",
    "\n",
    "Valid Examples:\n",
    "Sentence: \"The agreement in question involves number in nouns and reflexive pronouns and is syntactic rather than semantic in nature because grammatical number in English , like grammatical gender in languages such as French , is partly arbitrary .\"\n",
    "Triples: [(\"nouns\", \"conjunction\", \"reflexive pronouns\"), (\"grammatical gender\", \"feature-of\", \"languages\"), (\"french\", \"hyponym-of\", \"languages\")]\n",
    "\n",
    "Sentence: \"In this paper , a novel method to learn the intrinsic object structure for robust visual tracking is proposed .\"\n",
    "Triples: [(\"novel method\", \"used-for\", \"learn the intrinsic object structure\"), (\"intrinsic object structure\", \"used-for\", \"robust visual tracking\")]\n",
    "\n",
    "Apply to the text. Remember: ONLY use relations from the list.\"\"\"\n",
    "\n",
    "# --- Normalization Logic ---\n",
    "def normalize_entity(entity_text: str) -> str:\n",
    "    if not isinstance(entity_text, str):\n",
    "        return \"\"\n",
    "    \n",
    "    text = entity_text.strip().lower()\n",
    "    text = re.sub(r'^(a|an|the)\\s+', '', text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r'\\s*\\.$', '', text)\n",
    "    text = text.strip('\\'\"')\n",
    "    \n",
    "    return text if text else \"\"\n",
    "\n",
    "# Load dataset\n",
    "try:\n",
    "    with open(DATASET_PATH, 'r') as f:\n",
    "        data = json.load(f)\n",
    "except FileNotFoundError:\n",
    "    logging.error(f\"FATAL: Dataset not found at path: {DATASET_PATH}\")\n",
    "    print(f\"FATAL: Dataset not found at path: {DATASET_PATH}\")\n",
    "    exit()\n",
    "\n",
    "if NUM_EXAMPLES_TO_TEST:\n",
    "    data = data[:NUM_EXAMPLES_TO_TEST]\n",
    "\n",
    "print(f\"Loading sentence transformer model: {SENTENCE_MODEL_NAME}...\")\n",
    "sentence_model = SentenceTransformer(SENTENCE_MODEL_NAME)\n",
    "print(\"Model loaded.\")\n",
    "\n",
    "# --- Benchmark Loop ---\n",
    "all_gold_triples, all_pred_triples, all_pred_strings, all_gold_strings = [], [], [], []\n",
    "item_details = []\n",
    "success_count = 0\n",
    "fail_count = 0\n",
    "failed_entries = []\n",
    "\n",
    "for i, item in enumerate(data):\n",
    "    text = item['text']\n",
    "    gold_triples_raw = item.get('triple_list', [])\n",
    "    \n",
    "    gold_triples = []\n",
    "    for subj, pred, obj in gold_triples_raw:\n",
    "        subj_norm = normalize_entity(subj)\n",
    "        obj_norm = normalize_entity(obj)\n",
    "        pred_norm = pred.strip().lower().replace(\" \", \"-\")\n",
    "        if subj_norm and obj_norm:\n",
    "            gold_triples.append((subj_norm, pred_norm, obj_norm))\n",
    "    gold_triples = sorted(list(set(gold_triples)))\n",
    "\n",
    "    logging.info(f\"\\n----- Processing Item {i+1}/{len(data)} -----\")\n",
    "    logging.info(f\"Text: {text}\")\n",
    "    print(f\"Processing Item {i+1}/{len(data)}...\")\n",
    "    \n",
    "    pred_triples = []\n",
    "    try:\n",
    "        response = kg.generate(\n",
    "            input_data=text, \n",
    "            context=CONTEXT\n",
    "        )\n",
    "        pred_triples_raw = response.relations if response and hasattr(response, 'relations') else []\n",
    "        logging.info(f\"KGGen Extraction: {pred_triples_raw}\")\n",
    "\n",
    "        corrected_triples_set = set()\n",
    "        for triple in pred_triples_raw:\n",
    "            if len(triple) != 3: continue\n",
    "            subj, pred, obj = triple\n",
    "            subj_norm = normalize_entity(subj)\n",
    "            obj_norm = normalize_entity(obj)\n",
    "            pred_norm = pred.strip().lower().replace(\" \", \"-\")\n",
    "            if subj_norm and obj_norm:\n",
    "                corrected_triples_set.add((subj_norm, pred_norm, obj_norm))\n",
    "\n",
    "        pred_triples = sorted(list(corrected_triples_set))\n",
    "        success_count += 1\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An API or parsing error occurred for item {i+1}: {e}\", exc_info=True)\n",
    "        pred_triples = []\n",
    "        fail_count += 1\n",
    "        failed_entries.append({\n",
    "            'index': i+1,\n",
    "            'text': text,\n",
    "            'gold_triples_raw': gold_triples_raw,\n",
    "            'error': str(e)\n",
    "        })\n",
    "\n",
    "    logging.info(f\"Gold (Normalized): {gold_triples}\")\n",
    "    logging.info(f\"Pred (Normalized): {pred_triples}\")\n",
    "\n",
    "    all_gold_triples.extend(gold_triples)\n",
    "    all_pred_triples.extend(pred_triples)\n",
    "\n",
    "    pred_str = \" | \".join(sorted([f\"{h} {r} {t}\" for h, r, t in set(pred_triples)]))\n",
    "    gold_str = \" | \".join(sorted([f\"{h} {r} {t}\" for h, r, t in set(gold_triples)]))\n",
    "    all_pred_strings.append(pred_str)\n",
    "    all_gold_strings.append(gold_str)\n",
    "\n",
    "    item_details.append({\n",
    "        'index': i+1,\n",
    "        'gold_list': set(gold_triples), 'pred_list': set(pred_triples),\n",
    "        'gold_str': gold_str, 'pred_str': pred_str,\n",
    "        'gold_triples': gold_triples, 'pred_triples': pred_triples\n",
    "    })\n",
    "\n",
    "# Write failed entries to JSON\n",
    "if failed_entries:\n",
    "    with open(FAILED_ENTRIES_PATH, 'w') as f:\n",
    "        json.dump(failed_entries, f, indent=4)\n",
    "    logging.info(f\"Failed entries written to {FAILED_ENTRIES_PATH}\")\n",
    "else:\n",
    "    logging.info(\"No failed entries.\")\n",
    "\n",
    "# Log processing counts\n",
    "logging.info(f\"\\n----- Processing Summary -----\")\n",
    "logging.info(f\"Successful items: {success_count}\")\n",
    "logging.info(f\"Failed items: {fail_count}\")\n",
    "print(f\"Successful items: {success_count}\")\n",
    "print(f\"Failed items: {fail_count}\")\n",
    "\n",
    "# --- Metrics Calculation ---\n",
    "logging.info(\"\\n----- FINAL METRICS -----\")\n",
    "\n",
    "# Strict Metrics\n",
    "gold_count = Counter(all_gold_triples)\n",
    "pred_count = Counter(all_pred_triples)\n",
    "tp = sum(min(gold_count[t], pred_count[t]) for t in gold_count)\n",
    "pred_total = len(all_pred_triples)\n",
    "gold_total = len(all_gold_triples)\n",
    "micro_p_strict = tp / pred_total if pred_total > 0 else 0.0\n",
    "micro_r_strict = tp / gold_total if gold_total > 0 else 0.0\n",
    "micro_f1_strict = 2 * micro_p_strict * micro_r_strict / \\\n",
    "    (micro_p_strict + micro_r_strict) if micro_p_strict + \\\n",
    "    micro_r_strict > 0 else 0.0\n",
    "\n",
    "item_ps_strict, item_rs_strict, item_f1s_strict = [], [], []\n",
    "for detail in item_details:\n",
    "    tp_item = len(detail['pred_list'] & detail['gold_list'])\n",
    "    p_item = tp_item / len(detail['pred_triples']) if detail['pred_triples'] else (1.0 if not detail['gold_triples'] else 0.0)\n",
    "    r_item = tp_item / len(detail['gold_triples']) if detail['gold_triples'] else (1.0 if not detail['pred_triples'] else 0.0)\n",
    "    if p_item + r_item == 0:\n",
    "        f1_item = 0.0\n",
    "    else:\n",
    "        f1_item = 2 * p_item * r_item / (p_item + r_item)\n",
    "    if not detail['pred_triples'] and not detail['gold_triples']:\n",
    "        f1_item = 1.0\n",
    "    item_ps_strict.append(p_item)\n",
    "    item_rs_strict.append(r_item)\n",
    "    item_f1s_strict.append(f1_item)\n",
    "    logging.info(\n",
    "        f\"Item {detail['index']} Strict: P={p_item:.4f}, R={r_item:.4f}, F1={f1_item:.4f}\")\n",
    "\n",
    "macro_p_strict = np.nanmean(item_ps_strict) if item_ps_strict else 0.0\n",
    "macro_r_strict = np.nanmean(item_rs_strict) if item_rs_strict else 0.0\n",
    "macro_f1_strict = np.nanmean(item_f1s_strict) if item_f1s_strict else 0.0\n",
    "\n",
    "# Base BERTScore (with IDF added for robustness)\n",
    "from bert_score import score as bert_score_compute\n",
    "\n",
    "if any(all_pred_strings) and any(all_gold_strings):\n",
    "    # Base BERTScore (P, R, F1) - Uses greedy max as per formulas\n",
    "    P_macro, R_macro, F1_macro = bert_score_compute(\n",
    "        all_pred_strings, all_gold_strings, lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=True\n",
    "    )\n",
    "    macro_p_bert = P_macro.mean().item()\n",
    "    macro_r_bert = R_macro.mean().item()\n",
    "    macro_f1_bert = F1_macro.mean().item()\n",
    "\n",
    "    all_pred_concat = ' | '.join(filter(None, all_pred_strings))\n",
    "    all_gold_concat = ' | '.join(filter(None, all_gold_strings))\n",
    "    P_micro, R_micro, F1_micro = bert_score_compute(\n",
    "        [all_pred_concat], [all_gold_concat], lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=False\n",
    "    )\n",
    "    micro_p_bert = P_micro.item()\n",
    "    micro_r_bert = R_micro.item() if not np.isnan(R_micro.item()) else 0.0\n",
    "    micro_f1_bert = F1_micro.item() if not np.isnan(F1_micro.item()) else 0.0\n",
    "    \n",
    "    # Greedy BERTScore\n",
    "    item_gbs_p, item_gbs_r, item_gbs_f1 = [], [], []\n",
    "\n",
    "    for detail in item_details:\n",
    "        pred_strings = [f\"{h} {r} {t}\" for h, r, t in detail['pred_triples']]\n",
    "        gold_strings = [f\"{h} {r} {t}\" for h, r, t in detail['gold_triples']]\n",
    "\n",
    "        if not pred_strings and not gold_strings:\n",
    "            item_gbs_p.append(1.0)\n",
    "            item_gbs_r.append(1.0)\n",
    "            item_gbs_f1.append(1.0)\n",
    "            continue\n",
    "\n",
    "        if not pred_strings:  # Has gold, but no preds (all missed)\n",
    "            item_gbs_p.append(1.0)  # Vacuously true: all 0 preds are \"correct\"\n",
    "            item_gbs_r.append(0.0)\n",
    "            item_gbs_f1.append(0.0)\n",
    "            continue\n",
    "\n",
    "        if not gold_strings:  # Has preds, but no gold (all false pos)\n",
    "            item_gbs_p.append(0.0)\n",
    "            item_gbs_r.append(1.0)  # Vacuously true: all 0 golds are \"found\"\n",
    "            item_gbs_f1.append(0.0)\n",
    "            continue\n",
    "        \n",
    "        # G-BS-P (Precision): Avg. max similarity for each *predicted* triple\n",
    "        P_gbs, _, _ = bert_score_compute(\n",
    "            pred_strings,\n",
    "            [gold_strings] * len(pred_strings),  # Compare each pred to ALL golds\n",
    "            lang=\"en\", verbose=False,\n",
    "            model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        gbs_p = P_gbs.mean().item()\n",
    "\n",
    "        # G-BS-R (Recall): Avg. max similarity for each *gold* triple\n",
    "        P_recall, _, _ = bert_score_compute(  # Capture the first value (Precision)\n",
    "            gold_strings,\n",
    "            [pred_strings] * len(gold_strings),  # Compare each gold to ALL preds\n",
    "            lang=\"en\", verbose=False,\n",
    "            model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        gbs_r = P_recall.mean().item()  # Use the Precision from the flipped call\n",
    "        \n",
    "        gbs_f1 = 0.0\n",
    "        if (gbs_p + gbs_r) > 0:\n",
    "            gbs_f1 = 2 * gbs_p * gbs_r / (gbs_p + gbs_r)\n",
    "\n",
    "        item_gbs_p.append(gbs_p)\n",
    "        item_gbs_r.append(gbs_r)\n",
    "        item_gbs_f1.append(gbs_f1)\n",
    "\n",
    "    # Macro averages (mean over items)\n",
    "    macro_gbs_p = np.nanmean(item_gbs_p) if item_gbs_p else 0.0\n",
    "    macro_gbs_r = np.nanmean(item_gbs_r) if item_gbs_r else 0.0\n",
    "    macro_gbs_f1 = np.nanmean(item_gbs_f1) if item_gbs_f1 else 0.0\n",
    "\n",
    "    # Micro for G-BS: Flatten all triples and compute global greedy\n",
    "    all_pred_triple_strs = [f\"{h} {r} {t}\" for triples in item_details for h, r, t in triples['pred_triples']]\n",
    "    all_gold_triple_strs = [f\"{h} {r} {t}\" for triples in item_details for h, r, t in triples['gold_triples']]\n",
    "    \n",
    "    if all_pred_triple_strs and all_gold_triple_strs:\n",
    "        # Micro P: Avg max sim for each pred vs all golds\n",
    "        P_micro_g, _, _ = bert_score_compute(\n",
    "            all_pred_triple_strs,\n",
    "            [all_gold_triple_strs] * len(all_pred_triple_strs),\n",
    "            lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        micro_gbs_p = np.nanmean(P_micro_g.cpu().numpy())\n",
    "\n",
    "        # Micro R: Avg max sim for each gold vs all preds\n",
    "        P_micro_r, _, _ = bert_score_compute(  # Capture the first value (Precision)\n",
    "            all_gold_triple_strs,\n",
    "            [all_pred_triple_strs] * len(all_gold_triple_strs),\n",
    "            lang=\"en\", verbose=False, model_type=\"roberta-large\", idf=True\n",
    "        )\n",
    "        micro_gbs_r = np.nanmean(P_micro_r.cpu().numpy())\n",
    "        \n",
    "        micro_gbs_f1 = 0.0\n",
    "        if (micro_gbs_p + micro_gbs_r) > 0:\n",
    "            micro_gbs_f1 = 2 * micro_gbs_p * micro_gbs_r / (micro_gbs_p + micro_gbs_r)\n",
    "    else:\n",
    "        micro_gbs_p, micro_gbs_r, micro_gbs_f1 = 0.0, 0.0, 0.0\n",
    "    \n",
    "else:\n",
    "    # All zeros for empties\n",
    "    macro_p_bert, macro_r_bert, macro_f1_bert = 0.0, 0.0, 0.0\n",
    "    micro_p_bert, micro_r_bert, micro_f1_bert = 0.0, 0.0, 0.0\n",
    "    macro_gbs_p, macro_gbs_r, macro_gbs_f1 = 0.0, 0.0, 0.0\n",
    "    micro_gbs_p, micro_gbs_r, micro_gbs_f1 = 0.0, 0.0, 0.0\n",
    "\n",
    "# G-BERTScore (Hungarian Matching) from EXPLAGRAPHS\n",
    "def get_g_bert_score(all_gold_edges, all_pred_edges, idf=False):\n",
    "    references = []\n",
    "    candidates = []\n",
    "    ref_cand_index = {}\n",
    "    for graph_idx, (gold_edges, pred_edges) in enumerate(zip(all_gold_edges, all_pred_edges)):\n",
    "        for gold_edge in gold_edges:\n",
    "            for pred_edge in pred_edges:\n",
    "                references.append(gold_edge)\n",
    "                candidates.append(pred_edge)\n",
    "                ref_cand_index[(graph_idx, gold_edge, pred_edge)] = len(references) - 1\n",
    "\n",
    "    if not references:\n",
    "        return np.zeros(len(all_gold_edges)), np.zeros(len(all_gold_edges)), np.zeros(len(all_gold_edges))\n",
    "\n",
    "    _, _, bs_F1 = bert_score_compute(candidates, references, lang='en', verbose=False, model_type=\"roberta-large\", idf=idf)\n",
    "    bs_F1 = bs_F1.cpu().numpy()\n",
    "\n",
    "    precisions, recalls, f1s = [], [], []\n",
    "    for graph_idx, (gold_edges, pred_edges) in enumerate(zip(all_gold_edges, all_pred_edges)):\n",
    "        num_gold = len(gold_edges)\n",
    "        num_pred = len(pred_edges)\n",
    "        if num_gold == 0 and num_pred == 0:\n",
    "            precisions.append(1.0)\n",
    "            recalls.append(1.0)\n",
    "            f1s.append(1.0)\n",
    "            continue\n",
    "        if num_pred == 0:\n",
    "            precisions.append(1.0)\n",
    "            recalls.append(0.0)\n",
    "            f1s.append(0.0)\n",
    "            continue\n",
    "        if num_gold == 0:\n",
    "            precisions.append(0.0)\n",
    "            recalls.append(1.0)\n",
    "            f1s.append(0.0)\n",
    "            continue\n",
    "\n",
    "        score_matrix = np.zeros((num_gold, num_pred))\n",
    "        for i, gold_edge in enumerate(gold_edges):\n",
    "            for j, pred_edge in enumerate(pred_edges):\n",
    "                idx = ref_cand_index.get((graph_idx, gold_edge, pred_edge))\n",
    "                if idx is not None:\n",
    "                    score_matrix[i, j] = bs_F1[idx]\n",
    "\n",
    "        row_ind, col_ind = linear_sum_assignment(score_matrix, maximize=True)\n",
    "        matched_sim = score_matrix[row_ind, col_ind]\n",
    "\n",
    "        sample_precision = matched_sim.sum() / num_pred\n",
    "        sample_recall = matched_sim.sum() / num_gold\n",
    "        sample_f1 = 2 * sample_precision * sample_recall / (sample_precision + sample_recall) if sample_precision + sample_recall > 0 else 0.0\n",
    "\n",
    "        precisions.append(sample_precision)\n",
    "        recalls.append(sample_recall)\n",
    "        f1s.append(sample_f1)\n",
    "\n",
    "    return np.array(precisions), np.array(recalls), np.array(f1s)\n",
    "\n",
    "all_gold_edges = [[f\"{h} {r} {t}\" for h, r, t in detail['gold_triples']] for detail in item_details]\n",
    "all_pred_edges = [[f\"{h} {r} {t}\" for h, r, t in detail['pred_triples']] for detail in item_details]\n",
    "\n",
    "precisions_h, recalls_h, f1s_h = get_g_bert_score(all_gold_edges, all_pred_edges, idf=False)\n",
    "\n",
    "macro_gbs_p_h = np.nanmean(precisions_h) if len(precisions_h) > 0 else 0.0\n",
    "macro_gbs_r_h = np.nanmean(recalls_h) if len(recalls_h) > 0 else 0.0\n",
    "macro_gbs_f1_h = np.nanmean(f1s_h) if len(f1s_h) > 0 else 0.0\n",
    "\n",
    "# Micro for G-BERTScore: Sum matched similarities from per-sample computations\n",
    "total_matched_sum = 0.0\n",
    "for i in range(len(precisions_h)):\n",
    "    total_matched_sum += precisions_h[i] * len(all_pred_edges[i])\n",
    "micro_gbs_p_h = total_matched_sum / pred_total if pred_total > 0 else 0.0\n",
    "micro_gbs_r_h = total_matched_sum / gold_total if gold_total > 0 else 0.0\n",
    "micro_gbs_f1_h = 2 * micro_gbs_p_h * micro_gbs_r_h / (micro_gbs_p_h + micro_gbs_r_h) if micro_gbs_p_h + micro_gbs_r_h > 0 else 0.0\n",
    "\n",
    "# Soft Semantic Score\n",
    "threshold = 0.8\n",
    "global_matched_preds, global_matched_golds = 0, 0\n",
    "item_ps_soft, item_rs_soft, item_f1s_soft = [], [], []\n",
    "\n",
    "for detail in item_details:\n",
    "    pred_triples, gold_triples = detail['pred_triples'], detail['gold_triples']\n",
    "    if not pred_triples and not gold_triples:\n",
    "        item_ps_soft.append(1.0)\n",
    "        item_rs_soft.append(1.0)\n",
    "        item_f1s_soft.append(1.0)\n",
    "        continue\n",
    "\n",
    "    pred_strings = [f\"{h} {r} {t}\" for h, r, t in pred_triples]\n",
    "    gold_strings = [f\"{h} {r} {t}\" for h, r, t in gold_triples]\n",
    "\n",
    "    if not pred_strings or not gold_strings:\n",
    "        matched_preds, matched_golds = 0, 0\n",
    "    else:\n",
    "        pred_embs = sentence_model.encode(pred_strings, convert_to_tensor=True).cpu().numpy()\n",
    "        gold_embs = sentence_model.encode(gold_strings, convert_to_tensor=True).cpu().numpy()\n",
    "        \n",
    "        # Cosine similarity matrix (normalize to [0,1] for matching)\n",
    "        sim_matrix = np.dot(pred_embs, gold_embs.T) / (np.linalg.norm(pred_embs, axis=1)[:, np.newaxis] * np.linalg.norm(gold_embs, axis=1))\n",
    "        sim_matrix = np.maximum(sim_matrix, 0)  # Clip negatives\n",
    "        \n",
    "        # Hungarian for optimal assignment (maximize similarity)\n",
    "        row_ind, col_ind = linear_sum_assignment(sim_matrix, maximize=True)\n",
    "        matched_sim = sim_matrix[row_ind, col_ind]\n",
    "        \n",
    "        # Count matches above threshold\n",
    "        matched_preds = sum(1 for s in matched_sim if s > threshold)\n",
    "        matched_golds = matched_preds  # Symmetric in bipartite\n",
    "\n",
    "    p_item = matched_preds / len(pred_triples) if pred_triples else (1.0 if not gold_triples else 0.0)\n",
    "    r_item = matched_golds / len(gold_triples) if gold_triples else (1.0 if not pred_triples else 0.0)\n",
    "    f1_item = 2 * p_item * r_item / (p_item + r_item) if p_item + r_item > 0 else 0.0\n",
    "\n",
    "    item_ps_soft.append(p_item)\n",
    "    item_rs_soft.append(r_item)\n",
    "    item_f1s_soft.append(f1_item)\n",
    "    global_matched_preds += matched_preds\n",
    "    global_matched_golds += matched_golds\n",
    "    logging.info(f\"Item {detail['index']} Semantic: P={p_item:.4f}, R={r_item:.4f}, F1={f1_item:.4f}\")\n",
    "\n",
    "micro_p_soft = global_matched_preds / pred_total if pred_total > 0 else 0.0\n",
    "micro_r_soft = global_matched_golds / gold_total if gold_total > 0 else 0.0\n",
    "micro_f1_soft = 2 * micro_p_soft * micro_r_soft / (micro_p_soft + micro_r_soft) if micro_p_soft + micro_r_soft > 0 else 0.0\n",
    "\n",
    "macro_p_soft = np.nanmean(item_ps_soft) if item_ps_soft else 0.0\n",
    "macro_r_soft = np.nanmean(item_rs_soft) if item_rs_soft else 0.0\n",
    "macro_f1_soft = np.nanmean(item_f1s_soft) if item_f1s_soft else 0.0\n",
    "\n",
    "# --- Final Output ---\n",
    "output_str = f\"\"\"\n",
    "Benchmark Results:\n",
    "---------------------------------\n",
    "Number of examples: {len(data)}\n",
    "Strict Averages:\n",
    "  - Macro: P={macro_p_strict:.4f}, R={macro_r_strict:.4f}, F1={macro_f1_strict:.4f}\n",
    "  - Micro: P={micro_p_strict:.4f}, R={micro_r_strict:.4f}, F1={micro_f1_strict:.4f}\n",
    "\n",
    "BERTScore Averages:\n",
    "  - Macro: P={macro_p_bert:.4f}, R={macro_r_bert:.4f}, F1={macro_f1_bert:.4f}\n",
    "  - Micro: P={micro_p_bert:.4f}, R={micro_r_bert:.4f}, F1={micro_f1_bert:.4f}\n",
    "\n",
    "Greedy BS Averages:\n",
    "  - Macro: P={macro_gbs_p:.4f}, R={macro_gbs_r:.4f}, F1={macro_gbs_f1:.4f}\n",
    "  - Micro: P={micro_gbs_p:.4f}, R={micro_gbs_r:.4f}, F1={micro_gbs_f1:.4f}\n",
    "\n",
    "G-BERTScore Averages:\n",
    "  - Macro: P={macro_gbs_p_h:.4f}, R={macro_gbs_r_h:.4f}, F1={macro_gbs_f1_h:.4f}\n",
    "  - Micro: P={micro_gbs_p_h:.4f}, R={micro_gbs_r_h:.4f}, F1={micro_gbs_f1_h:.4f}\n",
    "\n",
    "Semantic Score Averages (Threshold: {threshold}):\n",
    "  - Macro: P={macro_p_soft:.4f}, R={macro_r_soft:.4f}, F1={macro_f1_soft:.4f}\n",
    "  - Micro: P={micro_p_soft:.4f}, R={micro_r_soft:.4f}, F1={micro_f1_soft:.4f}\n",
    "\"\"\"\n",
    "print(output_str)\n",
    "logging.info(output_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kggen_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
